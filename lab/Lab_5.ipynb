{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DT8060\n",
        "## Raffaello Baluyot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOahmM5VnGY8"
      },
      "source": [
        "# Lab 5 - Neural Networks\n",
        "\n",
        "In this lab we will use a pre-trained neural network that is designed for image classification. We will show you a number of different visualization options and allow you to play around with them yourself to gain a greater understanding.\n",
        "Likewise, we will show you how LIME is used for images as well.\n",
        "\n",
        "You are then tasked to use the techniques on another model to try and explain how it is working, and to investigate if there is any wrong doings in its predictions. Remember to also look into the dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_JbspTqIIEg"
      },
      "source": [
        "### Package import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ8qOe33nF9l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as c_map\n",
        "import matplotlib\n",
        "from IPython.display import Image, display\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import PIL\n",
        "#import os\n",
        "#os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 as Model\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tf_keras_vis.saliency import Saliency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHPVRVRscGbI"
      },
      "source": [
        "### Intializations and loading data\n",
        "\n",
        "The easiest approach to using the data for the lab, is to download the data folder (lab5_data) from the lab assignment page on blackboard. Reupload them to your root folder on your google drive. The cell below mounts your google drive to reach those uploaded files later. If you choose to do something else, you are free to do so but remember to update the file paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkyY5F4wTg3b"
      },
      "outputs": [],
      "source": [
        "basepath = \"../data/lab5_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MewU17G4IYaS"
      },
      "source": [
        "## Neural Network Image Activation visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8NIys_BcE4t"
      },
      "outputs": [],
      "source": [
        "model = Model(weights='imagenet', include_top=True)\n",
        "model.summary()\n",
        "\n",
        "# Image titles\n",
        "image_titles = ['Goldfish', 'Bear', 'Triceratops']\n",
        "GDRIVE_DIR = '/content/gdrive'\n",
        "# Load images and Convert them to a Numpy array\n",
        "img1 = load_img(os.path.join(basepath, 'goldfish.jpg'), target_size=(224, 224))\n",
        "img2 = load_img(os.path.join(basepath, 'bear.jpg'), target_size=(224, 224))\n",
        "img3 = load_img(os.path.join(basepath, 'triceratops.jpg'), target_size=(224, 224))\n",
        "#img1 = load_img('goldfish.JPG', target_size=(224, 224))\n",
        "#img2 = load_img('bear.jpg', target_size=(224, 224))\n",
        "#img3 = load_img('triceratops.jpg', target_size=(224, 224))\n",
        "images = np.asarray([np.array(img1), np.array(img2), np.array(img3)])\n",
        "\n",
        "# Preparing input data for VGG16\n",
        "X = preprocess_input(images)\n",
        "\n",
        "# Rendering\n",
        "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "for i, title in enumerate(image_titles):\n",
        "    ax[i].set_title(title, fontsize=16)\n",
        "    ax[i].imshow(images[i])\n",
        "    ax[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm1smeVOcvCu"
      },
      "source": [
        "Boilerplate code for the tf-Keras-vis package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6d_oRNUczvY"
      },
      "outputs": [],
      "source": [
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "\n",
        "def score_function(output):\n",
        "    # This version returns the result for the corresponding class from the\n",
        "    # prediction of each image we are to show.\n",
        "    # output[x][y] -> x = which image, y = which class in the imagenet labels\n",
        "    return (output[0][1], output[1][294], output[2][51])\n",
        "\n",
        "# Alters the softmax output to a linear output\n",
        "replace2linear = ReplaceToLinear()\n",
        "\n",
        "# The actual labels to each category that we investigate, i.e. the goldfish,\n",
        "# bear, and triceratops label indices.\n",
        "score = CategoricalScore([1, 294, 51])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvSg4eKwcjMh"
      },
      "source": [
        "Saliency map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJhivqBOckdC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "\n",
        "# Create Saliency object.\n",
        "saliency = Saliency(model,\n",
        "                    model_modifier=replace2linear,\n",
        "                    clone=True)\n",
        "\n",
        "# Generate saliency map\n",
        "saliency_map = saliency(score, X)\n",
        "\n",
        "# Render\n",
        "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "for i, title in enumerate(image_titles):\n",
        "    ax[i].set_title(title, fontsize=16)\n",
        "    ax[i].imshow(saliency_map[i], cmap='jet')\n",
        "    ax[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-uUeNHwevhu"
      },
      "source": [
        "Smooth Saliency map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaVeCjZcexPu"
      },
      "outputs": [],
      "source": [
        "# Generate saliency map with smoothing that reduce noise by adding noise\n",
        "saliency = Saliency(model,\n",
        "                    model_modifier=replace2linear,\n",
        "                    clone=True)\n",
        "\n",
        "saliency_map = saliency(score,\n",
        "                        X,\n",
        "                        smooth_samples=20, # The number of calculating gradients iterations.\n",
        "                        smooth_noise=0.20) # noise spread level.\n",
        "\n",
        "# Render\n",
        "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "for i, title in enumerate(image_titles):\n",
        "    ax[i].set_title(title, fontsize=14)\n",
        "    ax[i].imshow(saliency_map[i], cmap='jet')\n",
        "    ax[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LorXmTtHe6Ue"
      },
      "source": [
        "GRAD-Cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46jOIqh_e7ik"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "\n",
        "# Create Gradcam object\n",
        "gradcam = Gradcam(model,\n",
        "                  model_modifier=replace2linear,\n",
        "                  clone=True)\n",
        "\n",
        "# Generate heatmap with GradCAM\n",
        "cam = gradcam(score,\n",
        "              X,\n",
        "              penultimate_layer=-1)\n",
        "\n",
        "# Render\n",
        "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "for i, title in enumerate(image_titles):\n",
        "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
        "    ax[i].set_title(title, fontsize=16)\n",
        "    ax[i].imshow(images[i])\n",
        "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5) # overlay\n",
        "    ax[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydHqSlBKfBaz"
      },
      "source": [
        "GRAD-CAM++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkKQWQjLfC-M"
      },
      "outputs": [],
      "source": [
        "from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n",
        "\n",
        "# Create GradCAM++ object\n",
        "gradcam = GradcamPlusPlus(model,\n",
        "                          model_modifier=replace2linear,\n",
        "                          clone=True)\n",
        "\n",
        "# Generate heatmap with GradCAM++\n",
        "cam = gradcam(score,\n",
        "              X,\n",
        "              penultimate_layer=-1)\n",
        "\n",
        "# Render\n",
        "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "for i, title in enumerate(image_titles):\n",
        "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
        "    ax[i].set_title(title, fontsize=16)\n",
        "    ax[i].imshow(images[i])\n",
        "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "    ax[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hth8JAAHfJmh"
      },
      "source": [
        "(Faster) ScoreCAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5KkiVBFfK8g"
      },
      "outputs": [],
      "source": [
        "# Fast ScoreCAM\n",
        "from tf_keras_vis.scorecam import Scorecam\n",
        "\n",
        "# Create ScoreCAM object\n",
        "scorecam = Scorecam(model, model_modifier=replace2linear)\n",
        "\n",
        "# Generate heatmap with Faster-ScoreCAM\n",
        "cam = scorecam(score,\n",
        "               X,\n",
        "               penultimate_layer=-1,\n",
        "               max_N=10)\n",
        "\n",
        "# Render\n",
        "f, ax = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "for i, title in enumerate(image_titles):\n",
        "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
        "    ax[i].set_title(title, fontsize=16)\n",
        "    ax[i].imshow(images[i])\n",
        "    ax[i].imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "    ax[i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmqdN_cHiu_5"
      },
      "source": [
        "These visualization techniques try to highlight which pixels provide the most importance to the prediction of the image.\n",
        "\n",
        "What's you view of the techniques? Are they useful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lf71vdLi9xE"
      },
      "source": [
        "The visualization of each results does explain which parts does the model look more. It is a very intuitive explanation of what happens with the image. This can help when it comes to seeing if the predictor has a whole view of the image. For example, most techniques seems to show the whole fish as being considered while the bear and triceratops only covers certain parts depending on the technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbLQf_FfftYE"
      },
      "source": [
        "## Neural Network Layer Dissections/visualizations\n",
        "We change the scoring from before to work in this context. The following score, when used, returns the value from the 3rd filter in the **block5_conv3** layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK4kMr4Bfvm7"
      },
      "outputs": [],
      "source": [
        "from tf_keras_vis.utils.model_modifiers import ExtractIntermediateLayer, ReplaceToLinear\n",
        "\n",
        "layer_name = 'block5_conv3' # The target layer that is the last layer of VGG16.\n",
        "\n",
        "# This instance constructs new model whose output is replaced to `block5_conv3` layer's output.\n",
        "extract_intermediate_layer = ExtractIntermediateLayer(index_or_name=layer_name)\n",
        "# This instance modify the model's last activation function to linear one.\n",
        "replace2linear = ReplaceToLinear()\n",
        "\n",
        "filter_number = 3\n",
        "score = CategoricalScore(filter_number)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjBWK7EdgI7h"
      },
      "source": [
        "Create an activation maximum instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhfILaStgL68"
      },
      "outputs": [],
      "source": [
        "from tf_keras_vis.activation_maximization import ActivationMaximization\n",
        "\n",
        "activation_maximization = ActivationMaximization(model,\n",
        "                                                 # Please note that `extract_intermediate_layer` has to come before `replace2linear`.\n",
        "                                                 model_modifier=[extract_intermediate_layer, replace2linear],\n",
        "                                                 clone=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg7oFSaPhbUK"
      },
      "source": [
        "Visualizes the 63rd convolutional layer. This takes a while to run, so if you're in a hurry you can skip it for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPnnt9qrgiJC"
      },
      "outputs": [],
      "source": [
        "from tf_keras_vis.activation_maximization.callbacks import Progress\n",
        "\n",
        "# Generate maximized activation\n",
        "activations = activation_maximization(score,\n",
        "                                      callbacks=[Progress()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0qNL9uOjGJw"
      },
      "outputs": [],
      "source": [
        "# Render the filter from above\n",
        "f, ax = plt.subplots(figsize=(4, 4))\n",
        "ax.imshow(tf.cast(activations[0], np.uint8))\n",
        "ax.set_title('filter[{:03d}]'.format(filter_number), fontsize=16)\n",
        "ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrLwtfRxkyNg"
      },
      "source": [
        "These techniques instead tries to visualize the features each layer is seeing from an image. How would you describe the above? Is it useful for you to understand this filter? Can you understand it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP923gpSk9ok"
      },
      "source": [
        "The results is very hard to understand. There's not a single pattern recognizable to the human eye. It is closer to an abstract art."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUkx9C-4iKo4"
      },
      "source": [
        "## LIME\n",
        "The LIME library also works well on images, to see where the model puts its interest in the images it predicts upon.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGS2d5nsiWCh"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "from lime import lime_image\n",
        "\n",
        "# creating the explainer object\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "# You can view each individual image we have looked at before by\n",
        "# changing the index in the images[] variable below.\n",
        "explanation = explainer.explain_instance(images[2], model.predict\n",
        "                                         , top_labels=5,\n",
        "                                         hide_color=0, num_samples=1000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWcVAmUhmpIg"
      },
      "source": [
        "It is important that you run a large amount of samples, otherwise you could get misleading results. Try and lower the num_samples substantially above and see what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcQF8PNFlrEx"
      },
      "source": [
        "You can visualize with both the image as the background"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avRECvQ9lp1e"
      },
      "outputs": [],
      "source": [
        "from skimage.segmentation import mark_boundaries\n",
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
        "plt.imshow(mark_boundaries(temp, mask))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwr4BlgQmy_T"
      },
      "source": [
        "Or just the mask itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05Q97QApmU0d"
      },
      "outputs": [],
      "source": [
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
        "plt.imshow(mark_boundaries(temp, mask))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEMgSgiwgMWu"
      },
      "source": [
        "The result does not seem to match the image. The mask is showing the background rather than the foreground."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc8TMAbMj25x"
      },
      "source": [
        "# Assignment\n",
        "In your assignment, you are to use the given blackbox model on a dataset and explore if you can identify what the model has actually learned. The dataset is based on the Dogs and Wolves dataset from [Kaggle](https://www.kaggle.com/datasets/harishvutukuri/dogs-vs-wolves?resource=download). It has been significantly reduced and picked out images to only contain huskies and wolves. Some additional images have been added manually.\n",
        "\n",
        "Investigate the model using **some** of the techniques shown from **tf-keras-vis** and using the **LIME** library and try to explain what the model have actually learned. Using tf-keras-vis and its visualizations might require you to improve your tensorflow/keras skills, here is their website for guidande and examples if needed [website](https://keisen.github.io/tf-keras-vis-docs/index.html)\n",
        "\n",
        "Look into which images are correctly classified in the validation set and which aren't. Visualize which parts of the images that are of importance for the model.\n",
        "\n",
        "What conclusions can you draw?\n",
        "\n",
        "What does the model really look for in the images?\n",
        "\n",
        "Show your steps, explain them, and reflect upon your results in cells below.\n",
        "\n",
        "You will find an example of how you can use and visualize the Saliency Maps on this dataset and model. Adapt accordingly for the remainder of the techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szOEnvJDUNJQ"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "## Black box training\n",
        "Training the model to be analyzed by the students."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT4vvPPanWzU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "training_path = os.path.join(basepath, 'husky_wolves/training/')\n",
        "validation_path = os.path.join(basepath, 'husky_wolves/validation/')\n",
        "\n",
        "image_gen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = image_gen.flow_from_directory(directory=training_path,\n",
        "                                                target_size=(256,256),\n",
        "                                                batch_size=20,\n",
        "                                                shuffle=True,\n",
        "                                                seed=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbTbx5WoKeDS"
      },
      "outputs": [],
      "source": [
        "def get_model(input_shape):\n",
        "    \"\"\"\n",
        "    This function should build and compile a CNN model according to the above specification,\n",
        "    using the functional API. Your function should return the model.\n",
        "    \"\"\"\n",
        "    input_layer = Input(input_shape)\n",
        "    h = Conv2D(filters=8, kernel_size=(8,8), padding='SAME', activation='relu')(input_layer)\n",
        "    h = MaxPool2D((2,2))(h)\n",
        "    h = Conv2D(4, (4,4), padding='SAME', activation='relu')(h)\n",
        "    h = MaxPool2D((2,2))(h)\n",
        "    Flatten_layer = Flatten()(h)\n",
        "    h = Dense(16, activation='relu')(Flatten_layer)\n",
        "    output_layer = Dense(2, activation='softmax')(h)\n",
        "    model = Model(inputs= input_layer, outputs = output_layer)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "    model.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics = [tf.keras.metrics.CategoricalAccuracy(name='accuracy')])\n",
        "    return model\n",
        "\n",
        "model = get_model((256, 256, 3))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dJy52Y6rKh9n"
      },
      "outputs": [],
      "source": [
        "model.fit(train_generator, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA9v_XhP5bpV"
      },
      "source": [
        "## Loading validation data, for you to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM8CAM3DsWHS"
      },
      "outputs": [],
      "source": [
        "images = np.asarray([load_img(os.path.join(validation_path, 'husky', 'husky_validation_{0}.jpg'.format(x)), target_size=(256,256)) for x in range(1,9)] + [load_img(os.path.join(validation_path, 'wolf', 'wolf_validation_{0}.jpg'.format(x)), target_size=(256,256)) for x in range(1,9)])\n",
        "labels = np.array([0 for x in range(8)] + [1 for x in range(8)])\n",
        "class_names = ['husky', 'wolf']\n",
        "images = images/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result of the model is already pretty bad. It's no better than random chance. From here it must be noted that the expectation is that the model will probably produce random results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XczPTGBstQoi"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "i = 0\n",
        "predictions = model.predict(images)\n",
        "for i in range(len(labels)):\n",
        "  if np.argmax(predictions[i]) == labels[i]:\n",
        "    count += 1\n",
        "print('validation accuracy %.3f'%(count/len(labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PljbUhiS5ks7"
      },
      "source": [
        "## Visualizing validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFP1umksYifa"
      },
      "outputs": [],
      "source": [
        "j = 1\n",
        "k = 10\n",
        "plt.figure(figsize=(18,6))\n",
        "for i in range(16):\n",
        "  if labels[i] == 0:\n",
        "    ax = plt.subplot(2,9,k)\n",
        "    k += 1\n",
        "  else:\n",
        "    ax = plt.subplot(2,9,j)\n",
        "    j+=1\n",
        "\n",
        "  plt.imshow((images[i]*255).astype('uint8'))\n",
        "  plt.title(class_names[labels[i]], color='green')\n",
        "  plt.text(128,300, class_names[np.argmax(predictions[i])], color='red', fontsize='large', horizontalalignment='center')\n",
        "  plt.text(128,350, 'validation[{0}]'.format(i), horizontalalignment='center')\n",
        "  plt.axis('off')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvpw6mMv5mL0"
      },
      "source": [
        "## Visualizing saliency maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXuwD-YXr3Xu"
      },
      "outputs": [],
      "source": [
        "# Generate saliency map with smoothing that reduce noise by adding noise\n",
        "\n",
        "score = CategoricalScore([0]*8+[1]*8)\n",
        "\n",
        "saliency = Saliency(model,\n",
        "                    model_modifier=replace2linear,\n",
        "                    clone=True)\n",
        "\n",
        "saliency_map = saliency(score,\n",
        "                        images)#,\n",
        "                        #smooth_samples=20, # The number of calculating gradients iterations.\n",
        "                        #smooth_noise=0.20) # noise spread level.\n",
        "\n",
        "# Render\n",
        "j = 1\n",
        "k = 10\n",
        "plt.figure(figsize=(18,6))\n",
        "for i in range(16):\n",
        "  if labels[i] == 0:\n",
        "    ax = plt.subplot(2,9,k)\n",
        "    k += 1\n",
        "  else:\n",
        "    ax = plt.subplot(2,9,j)\n",
        "    j+=1\n",
        "    #ax[i].set_title(title, fontsize=14)\n",
        "  plt.imshow(saliency_map[i], cmap='jet')\n",
        "  plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "685f3Y1Y5yFI"
      },
      "outputs": [],
      "source": [
        "# Generate saliency map with smoothing that reduce noise by adding noise\n",
        "\n",
        "score = CategoricalScore([0]*8+[1]*8)\n",
        "\n",
        "saliency = Saliency(model,\n",
        "                    model_modifier=replace2linear,\n",
        "                    clone=True)\n",
        "\n",
        "saliency_map = saliency(score,\n",
        "                        images,\n",
        "                        smooth_samples=20, # The number of calculating gradients iterations.\n",
        "                        smooth_noise=0.20) # noise spread level.\n",
        "\n",
        "# Render\n",
        "j = 1\n",
        "k = 10\n",
        "plt.figure(figsize=(18,6))\n",
        "for i in range(16):\n",
        "  if labels[i] == 0:\n",
        "    ax = plt.subplot(2,9,k)\n",
        "    k += 1\n",
        "  else:\n",
        "    ax = plt.subplot(2,9,j)\n",
        "    j+=1\n",
        "    #ax[i].set_title(title, fontsize=14)\n",
        "  plt.imshow(saliency_map[i], cmap='jet')\n",
        "  plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As seen from the saliency maps, the network does not entirely focus on the actual difference between the two images. The focus looks to be randomly distributed across the image per each label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Gradcam object\n",
        "gradcam = Gradcam(model,\n",
        "                  model_modifier=replace2linear,\n",
        "                  clone=True)\n",
        "\n",
        "# Generate heatmap with GradCAM\n",
        "cam = gradcam(score,\n",
        "              images,\n",
        "              penultimate_layer=-1)\n",
        "\n",
        "# Render\n",
        "j = 1\n",
        "k = 10\n",
        "plt.figure(figsize=(18,6))\n",
        "for i in range(16):\n",
        "  if labels[i] == 0:\n",
        "    ax = plt.subplot(2,9,k)\n",
        "    k += 1\n",
        "  else:\n",
        "    ax = plt.subplot(2,9,j)\n",
        "    j+=1\n",
        "    #ax[i].set_title(title, fontsize=14)\n",
        "  heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
        "  plt.imshow(images[i])\n",
        "  plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "  plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Gradcam object\n",
        "gradcam = GradcamPlusPlus(model,\n",
        "                          model_modifier=replace2linear,\n",
        "                          clone=True)\n",
        "\n",
        "# Generate heatmap with GradCAM\n",
        "cam = gradcam(score,\n",
        "              images,\n",
        "              penultimate_layer=-1)\n",
        "\n",
        "# Render\n",
        "j = 1\n",
        "k = 10\n",
        "plt.figure(figsize=(18,6))\n",
        "for i in range(16):\n",
        "  if labels[i] == 0:\n",
        "    ax = plt.subplot(2,9,k)\n",
        "    k += 1\n",
        "  else:\n",
        "    ax = plt.subplot(2,9,j)\n",
        "    j+=1\n",
        "    #ax[i].set_title(title, fontsize=14)\n",
        "  heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
        "  plt.imshow(images[i])\n",
        "  plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "  plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ScoreCAM object\n",
        "scorecam = Scorecam(model, model_modifier=replace2linear)\n",
        "\n",
        "# Generate heatmap with Faster-ScoreCAM\n",
        "cam = scorecam(score,\n",
        "               images,\n",
        "               penultimate_layer=-1,\n",
        "               max_N=4)\n",
        "\n",
        "# Render\n",
        "j = 1\n",
        "k = 10\n",
        "plt.figure(figsize=(18,6))\n",
        "for i in range(16):\n",
        "  if labels[i] == 0:\n",
        "    ax = plt.subplot(2,9,k)\n",
        "    k += 1\n",
        "  else:\n",
        "    ax = plt.subplot(2,9,j)\n",
        "    j+=1\n",
        "    #ax[i].set_title(title, fontsize=14)\n",
        "  heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
        "  plt.imshow(images[i])\n",
        "  plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "  plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The CAM results are also very varied. Some do not even have results, while some do have results but they are mostly in the background. The model is not really good at differentiating the two labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# creating the explainer object\n",
        "explainer = lime_image.LimeImageExplainer(verbose=False)\n",
        "\n",
        "results = []\n",
        "for i in range(len(images)):\n",
        "    explanation = explainer.explain_instance(images[i], model.predict\n",
        "                                            , top_labels=5,\n",
        "                                            hide_color=0, num_samples=1000)\n",
        "\n",
        "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
        "    results.append((temp, mask))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for temp, mask in results:\n",
        "    plt.imshow(mark_boundaries(temp, mask))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The LIME results are also the same, some do focus on the background and some focus on the foreground. That being said, given the results from the initial model (the demo), LIME seems to give uninuitive results regardless. So it is hard to draw conclusions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
