{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DT8060\n",
        "## Raffaello Baluyot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PGHiPcshMt0"
      },
      "source": [
        "# Lab 3 - Feature Importance and Global Surrogates\n",
        "In this lab we will use Feature Permutation to determine the feature importance of built models, both classification and regression models.\n",
        "\n",
        "First, you are asked to implement feature permutation by yourself on a neural network classifier trained on a breast cancer dataset, then to compare your permutation scores to the prebuilt methods that exist.\n",
        "Then, the same is to be performed on a neural network regressor.\n",
        "\n",
        "Finally, you are to build a surrogate model to try and explain/interpret how a support vector machine (SVM) model makes its decisions by training a decision tree based on the predictions from the SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx2aYxnD0Xom"
      },
      "source": [
        "## Package import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHRk6odC61Kx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import graphviz\n",
        "from sklearn import metrics, datasets, tree\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.datasets import load_breast_cancer, load_diabetes, load_iris\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdYTYnnQStrv"
      },
      "source": [
        "## Classifier\n",
        "Here, we provide a black box model using a Multi Layer Perceptron classifier that has been trained on the breast cancer dataset. Your task is to identify which features are most important for the model using feature permutation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sAD4Cg143_d"
      },
      "outputs": [],
      "source": [
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data['data'])\n",
        "y = data['target']\n",
        "X.columns = data['feature_names']\n",
        "X=(X-X.min())/(X.max()-X.min())\n",
        "\n",
        "#X.head()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10EaJIFixu0y"
      },
      "source": [
        "Load the black box model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTNquOzxxu0z"
      },
      "outputs": [],
      "source": [
        "f = open('../data/model_bc.pkl', 'rb')\n",
        "clf = pickle.load(f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvQ7MTmVxu0z"
      },
      "source": [
        "If loading the black box model does not work, run the cell below to train the model yourself. It might take a while..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pbjpq7xd-4V"
      },
      "outputs": [],
      "source": [
        "# # ONLY RUN IF YOU COULD NOT UPLOAD THE BLACK BOX MODELS\n",
        "\n",
        "# parameters = {'solver': ['lbfgs'], 'max_iter': [2000], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(1,20), 'random_state':[42]}\n",
        "# clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
        "# clf.fit(X_train, y_train)\n",
        "# clf = clf.best_estimator_\n",
        "# with open('model_bc.pkl', 'wb') as out:\n",
        "#     pickle.dump(clf, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHmNTrn5hHCx"
      },
      "source": [
        "### Todo\n",
        "Implement feature permutation for the **clf** model and use the models built in scorer. Remember to permute the features multiple times as the results will vary depending on how the data is permuted. Present the results as the average result with standard deviations for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdNgpxz0xu00"
      },
      "outputs": [],
      "source": [
        "# Implement feature permutation\n",
        "\n",
        "def permutation_importance_(model, X, y, k, scoring=None, seed=None):\n",
        "    scoring = model.score if scoring is None else scoring\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    base_score = scoring(X, y)\n",
        "\n",
        "    feature_importances = []\n",
        "\n",
        "    for feature_name in X.columns:\n",
        "        feature_scores = []\n",
        "        for _ in range(k):\n",
        "            perm_data = X.copy()\n",
        "            perm_data[feature_name] = rng.choice(\n",
        "                perm_data[feature_name],\n",
        "                size=len(X),\n",
        "                replace=False\n",
        "            )\n",
        "\n",
        "            feature_scores.append(scoring(perm_data, y))\n",
        "        feature_importances.append(feature_scores)\n",
        "\n",
        "    feature_importances = base_score - np.asarray(feature_importances).T\n",
        "\n",
        "    return feature_importances.mean(0), feature_importances.std(0)\n",
        "\n",
        "res = permutation_importance_(clf, X_test, y_test, k=100, seed=0)\n",
        "\n",
        "for i in res[0].argsort()[::-1]:\n",
        "        print(f'{X.columns[i]:<19} '\n",
        "              f'{res[0][i]:.3f}'\n",
        "              f' +/- {res[1][i]:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2xhjmUbxu00"
      },
      "source": [
        "Of course there already exist implemented functionality to perform feature permutation, for instance with **sklearn**.\n",
        "\n",
        "See if your own results from feature permutation aligns with the ones produced in the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYooKyBOSJ53"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "from sklearn.inspection import permutation_importance\n",
        "r = permutation_importance(clf, X_test, y_test,\n",
        "                           n_repeats=100,\n",
        "                           random_state=0)\n",
        "\n",
        "for i in r.importances_mean.argsort()[::-1]:\n",
        "        print(f'{X.columns[i]:<19} '\n",
        "              f'{r.importances_mean[i]:.3f}'\n",
        "              f' +/- {r.importances_std[i]:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HdM_mUSh0_-"
      },
      "source": [
        "### Reflection\n",
        "- What do the results above mean?\n",
        "- What conclusions/insights can we draw of the model based of off them?\n",
        "\n",
        "Based on the results, when the correlation of the high importance features are removed to the model, the performance of the model has a bigger degradation. This means that those with higher importance are being relied to by the model.\n",
        "\n",
        "Based on results, the features on `worst texture`, `worst radius`, `mean concave points`, and `worse concave points` are the most important features used by the model. Aside from importance, these features are also non-redundant this the model lost a lot of performance when they are shuffled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MPwGULtTMXq"
      },
      "source": [
        "## Regression\n",
        "The same task as before lies ahead of you, but in this instance we instead look towards the Multi Layer Perceptron for regression. This model has been trained on a diabetes dataset where the target variable is the disease progression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjkHH5OINIdX"
      },
      "outputs": [],
      "source": [
        "data = load_diabetes()\n",
        "X = pd.DataFrame(data['data'])\n",
        "y = data['target']\n",
        "X.columns = data['feature_names']\n",
        "\n",
        "X.head()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B12VLDS2xu01"
      },
      "source": [
        "Load the black box model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWsH4T7Lxu01"
      },
      "outputs": [],
      "source": [
        "f = open('../data/model_diab.pkl', 'rb')\n",
        "clf = pickle.load(f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElBHAhK6xu01"
      },
      "source": [
        "If loading the black box model does not work, run the cell below to train the model yourself. It might take a while..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8NGV0HPxu02"
      },
      "outputs": [],
      "source": [
        "# # ONLY RUN IF YOU COULD NOT UPLOAD THE BLACK BOX MODELS\n",
        "# parameters = {'solver': ['lbfgs'], 'max_iter': [2000], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(1,20), 'random_state':[42]}\n",
        "# clf = GridSearchCV(MLPRegressor(), parameters, n_jobs=-1)\n",
        "# clf.fit(X_train, y_train)\n",
        "# clf = clf.best_estimator_\n",
        "# # Stores the produced model for the students' later use\n",
        "# with open('model_diab.pkl', 'wb') as out:\n",
        "#     pickle.dump(clf, out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAStn0_DiPez"
      },
      "source": [
        "### Todo\n",
        "Implement feature permutation for the **clf** model and use the models built in scorer. Remember to permute the features multiple times as the results will vary. Present the results as the average result with standard deviations for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfCQSOOLxu02"
      },
      "outputs": [],
      "source": [
        "#Implement feature permutation\n",
        "res = permutation_importance_(clf, X_test, y_test, k=100, seed=0)\n",
        "\n",
        "for i in res[0].argsort()[::-1]:\n",
        "        print(f'{X.columns[i]:<19} '\n",
        "              f'{res[0][i]:.3f}'\n",
        "              f' +/- {res[1][i]:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZtTwBknzVSG"
      },
      "outputs": [],
      "source": [
        "r = permutation_importance(clf, X_test, y_test,\n",
        "                           n_repeats=100,\n",
        "                           random_state=0)\n",
        "\n",
        "for i in r.importances_mean.argsort()[::-1]:\n",
        "        print(f\"{X.columns[i]:<19} \"\n",
        "              f\"{r.importances_mean[i]:.3f}\"\n",
        "              f\" +/- {r.importances_std[i]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7uEZsRfiTXD"
      },
      "source": [
        "### Reflection\n",
        "- What do the results above mean?\n",
        "- What conclusions can we draw of the model based of off them?\n",
        "\n",
        "Based on the results, when the correlation of the high importance features are removed to the model, the performance of the model has a bigger degradation. This means that those with higher importance are being relied to by the model.\n",
        "\n",
        "Based on results, the features on `s5`, `s1`, `bmi`, and `s2` are the most important features used by the model. Aside from importance, these features are also non-redundant this the model lost a lot of performance when they are shuffled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl7lSV09xu02"
      },
      "source": [
        "**to be filled by student**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shmwg_0269H2"
      },
      "source": [
        "# Global Surrogate\n",
        "In this part of the lab you will be tasked to train Support Vector Machines (SVMs) on two given datasets. The SVMs are then to be subject for the global surrogate approach, where you will train CART models on the input data and the output from the SVMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AotByfpUQM6"
      },
      "source": [
        "## Classification\n",
        "\n",
        "Here, you create a SVM. THe SVM is then used to predict data which is then fed into the CART algorithm to train.\n",
        "This mimics the model from the SVM into the cart model, which is cheaper to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rELd-au4Lm_U"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data['data'])\n",
        "X.columns = data['feature_names']\n",
        "y = data['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78L1GAIXmuCF"
      },
      "outputs": [],
      "source": [
        "clf = SVC()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU5A0V8Jm2zs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "surrogate = DecisionTreeClassifier()\n",
        "surrogate.fit(X_train, clf.predict(X_train))\n",
        "y_pred = surrogate.predict(X_test)\n",
        "\n",
        "r2_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLXiSjHZn_Pt"
      },
      "source": [
        "Visualize the tree and try and see how the CART approximates the SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SThmXZWsxu03"
      },
      "outputs": [],
      "source": [
        "def display_tree(model, feature_names):\n",
        "    g = tree.export_graphviz(\n",
        "        model, \n",
        "        feature_names=feature_names,\n",
        "        filled=True\n",
        "    )\n",
        "\n",
        "    print(dict(zip(\n",
        "        feature_names,\n",
        "        model.feature_importances_\n",
        "    )))\n",
        "    return graphviz.Source(g)\n",
        "\n",
        "display_tree(surrogate, X_train.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70FUMLv1xu03"
      },
      "source": [
        "* How is the SVM approximated?\n",
        "* What features give the most importance to the model?\n",
        "* How does it operate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx6xGngwxu03"
      },
      "source": [
        "The SVM was approximated well given the result of the surrogate performance. The SVM was also approximated using a simpler tree given that there are only three labels and the features looks to be linearly separable.\n",
        "\n",
        "The `petal length` is considered as the most important feature as it's the one that was used by the tree that divides the data into smaller categories that reduce the entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb2Shw06USKs"
      },
      "source": [
        "## Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-GPh07hd1D-"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "data = load_diabetes()\n",
        "X = pd.DataFrame(data['data'])\n",
        "X.columns = data['feature_names']\n",
        "y = data['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DyAo6gxnN6T"
      },
      "outputs": [],
      "source": [
        "clf = SVR()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqqdq2agnRpQ"
      },
      "outputs": [],
      "source": [
        "surrogate = DecisionTreeRegressor()\n",
        "surrogate.fit(X_train, clf.predict(X_train))\n",
        "y_pred = surrogate.predict(X_test)\n",
        "\n",
        "r2_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieRc6LsgoKor"
      },
      "source": [
        "Visualize the tree and try and see how the CART approximates the SVR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVqA_hDqxu04"
      },
      "outputs": [],
      "source": [
        "#Visualize the tree\n",
        "display_tree(surrogate, X_train.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMvyYW3Pxu04"
      },
      "source": [
        "* How is the SVM approximated?\n",
        "* What features give the most importance to the model?\n",
        "* How does it operate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfZIoCqxxu04"
      },
      "source": [
        "The SVM was approximated well given the result of the surrogate performance. However, the model results is not really easy to interpret. This is pretty much the result of doing tree regressions. The tree create steps even in some linear relationships.\n",
        "\n",
        "The `s5`, `bmi`, and `bp` are considered as the most important feature as it's the one that was used by the tree that divides the data into smaller categories that reduce the entropy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydunn8DcWyz1"
      },
      "source": [
        "# Final reflections\n",
        "\n",
        "Reflect about your laboration. This includes but is not limited to the example points below.\n",
        "\n",
        "* How does the feature permutation give us insight to explain the model?\n",
        "* What conclusions can you draw from it?\n",
        "* The Global surrogate model takes another approach to explain the model. Is it useful?\n",
        "* How can you use the surrogate to explain your model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5ZYYM9gxu04"
      },
      "source": [
        "Feature permutation provides insight to the model by shuffling the features. This removes the correlation of the feature with the target value. Then the performance of the model on this shuffled features is compared with the original performance. Thus, the impact of the removal of the feature correlation with the target is measured.\n",
        "\n",
        "Feature permutation shows the features that are non-redundant and provides impact to the model. Unfortunately, given it's one by one nature, highly correlated features might not be seen as important by the model.\n",
        "\n",
        "Global surrogate models try to predict the model dynamics by using another interpretable model. It is useful given a model's certain complexity. Once the target model reached certain complexity, the surrogate model can also become complex, making it hard to interpret.\n",
        "\n",
        "Global surrogate can be used depending on which surrogate model was picked, as the surrogate model is used as proxy to understand the target model dynamics."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
