{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT8060\n",
    "## Raffaello Baluyot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Interpretable Models\n",
    "\n",
    "In this lab, we will venture through the simpler, interpretable, models Linear Regression and Decision Trees.\n",
    "In part 1, linear regression will be applied to a housing dataset with houses for sale in California. The weights and the effect of the weights will be investigated to see how each feature affects the model and how using subset of features makes the models become more interpretable but at what cost of the predictive performance.\n",
    "\n",
    "In part 2, decision trees will be created to model a car safety dataset, and we will investigate their performance and how interpretable they are, with the use of graphical aids such as **graphviz** and **dtreeviz**.\n",
    "\n",
    "In the 3rd and final part, a Naive Bayes classifier is given to on a spam dataset. A Naive Bayes classifier operates naively by seeing each feature as independent and use their probabilities to determine its class. You are to investigate the model that is produced to try and explain what dictates the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVaquKKilAx1"
   },
   "source": [
    "# Linear Regression\n",
    "In this part of the lab, we train a linear regression model on a dataset containing housing prices from California. The target value is the price of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P549YfozTOXv"
   },
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRbp1FLWdDTA"
   },
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0J1VnuRTHEP"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "yY1CIXchd7dC",
    "outputId": "b347f62b-150d-4c4d-bc46-cdd210ed8798"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SiCqCN5phUu"
   },
   "source": [
    "## Inspect the model\n",
    "Presenting the weights of each attribute in the model gives us an indication of the importance of the feature and its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "2s6djXwJw-E2",
    "outputId": "80129a3d-a56e-4d88-f0eb-aeca99efb645"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Weight'] = np.append(lr.intercept_,lr.coef_)\n",
    "df.index=['Intercept'] + data['feature_names']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to show the distribution of the data, we provide a boxplot below so you see the scale for each of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame(X_train, columns=data['feature_names'])\n",
    "sns.boxplot(data=x_df, orient='h').set(ylabel='Features', xlabel='Weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJ2cLjccTg7o"
   },
   "source": [
    "### Feature Effect\n",
    "\n",
    "To visualize how each feature affects the prediction from the linear regression model we can look at the weights and its variance, but if we have a large difference in feature values it could be hard to interpret.\n",
    "\n",
    "If we instead calculate the effect of each feature and visualize that, we get a better indication of how the features affect the predictions.\n",
    "\n",
    "The effects of a feature on a single data instance is calculated as $$\\text{effect}_{j}^{(i)}=w_{j}x_{j}^{(i)}$$\n",
    "where $w_{j}$ is the weight of feature $j$, and $x_{j}^{(i)}$ is the value of feature $j$ in data instance $i$.\n",
    "\n",
    "We visualize the effect using boxplots, which show the median values (the line inside the box), the 1st to 3rd quartiles (the actual boxes), and the outliers (the dots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "x8cjhZlrTiN5",
    "outputId": "d98b010f-85d9-4e63-8ea4-44c6cd871e51"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 9]\n",
    "sns.set_theme(rc={\"figure.dpi\": 96})\n",
    "\n",
    "weights = lr.coef_\n",
    "effects = np.multiply(X_train, weights)\n",
    "\n",
    "df = pd.DataFrame(effects, columns=data['feature_names'])\n",
    "sns.boxplot(data=df, orient='h').set(title='Feature Effect', ylabel='Features', xlabel='Effect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ38REOcsJ5J"
   },
   "source": [
    "We can also show how the feature effects of single instances. This is a good indicator of the data, especially when it is plotted in combination with the effects of the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "RkkwGUYoroE8",
    "outputId": "fb6c52ef-0085-4341-cb34-6a735edc393a"
   },
   "outputs": [],
   "source": [
    "instance = np.multiply(X_train[10], weights)\n",
    "fix, ax = plt.subplots(1,1)\n",
    "plot = sns.boxplot(data=df, orient='h')\n",
    "plot = plot.set_title('Feature Effect')\n",
    "y_axis = [y for y in range(8)]\n",
    "\n",
    "ax.scatter(instance, y_axis, marker='x', color='r', s=70, zorder=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the results from the above plots? What does it mean if we compare the effect from the latitude and logitude features? What is their effect on the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer the question above here.**\n",
    "\n",
    "In general, the **latitude** and **longitude** features have the largest impact on then model result. The higher **latitude** means lower price (northen areas are cheaper) and the higher **longitude** means means more expensive (western areas are more expensive). Given that the model also gives significantly higher weights to these two features means that location is one of the main factors when it comes to house pricing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_eIXIItTiW9"
   },
   "source": [
    "### Lasso\n",
    "The more features you have, the more you affect the complexity of the linear regression model. It is an interpretable model when we have a modest number of features. When we increase the amount of features the interpretability deteriorates. But, how do we decide which features to discard? \n",
    "\n",
    "Using a domain expert is often a good choice. Likewise, identifying which features that have the highest correlation with the target could be a way to go.\n",
    "\n",
    "An automated approach called Lasso investigates which features provide the most for the model.\n",
    "\n",
    "In this example below, we investigate which two features provides the better model using Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2YMCjO9Tj0U",
    "outputId": "b992fcd2-ee31-45c8-dc17-b51098a786b5"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# threshold=-np.inf forces Lasso to pick the number of features specified\n",
    "sel_ = SelectFromModel(LinearRegression(), threshold=-np.inf, max_features=2)\n",
    "sel_.fit(X_train, y_train)\n",
    "sel_.get_support()\n",
    "\n",
    "X_train_selected = sel_.transform(X_train)\n",
    "X_test_selected = sel_.transform(X_test)\n",
    "lr_selected = LinearRegression()\n",
    "lr_selected.fit(X_train_selected, y_train)\n",
    "y_pred = lr_selected.predict(X_train_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6SI17feHx1j"
   },
   "source": [
    "Identify which models are better performing, by adapting the code from above, for each number of features ranging from 1 to 8 and store them for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onqKqGt_GbSC"
   },
   "outputs": [],
   "source": [
    "import dataclasses as dc\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "@dc.dataclass\n",
    "class TrainResult:\n",
    "    model: LinearRegression\n",
    "    selector: SelectFromModel\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_selected = self.selector.transform(X)\n",
    "        return self.model.predict(X_selected)\n",
    "    \n",
    "    def eval(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return mean_squared_error(y, pred)\n",
    "    \n",
    "    def plot_feature_effects(self, X):\n",
    "        X_selected = self.selector.transform(X)\n",
    "        effects = np.multiply(X_selected, self.model.coef_)\n",
    "        idx_selected = self.selector.transform(np.arange(self.selector.n_features_in_)[None, :])[0]\n",
    "        cols_selected = [data[\"feature_names\"][i] for i in idx_selected]\n",
    "        sns.boxplot(data=pd.DataFrame(effects, columns=cols_selected), orient='h')\n",
    "        plt.title('Feature Effect')\n",
    "        plt.show()\n",
    "\n",
    "def train_by_n_features(n_features):\n",
    "    sel_ = SelectFromModel(LinearRegression(), threshold=-np.inf, max_features=n_features)\n",
    "    sel_.fit(X_train, y_train)\n",
    "    sel_.get_support()\n",
    "\n",
    "    X_train_selected = sel_.transform(X_train)\n",
    "    lr_selected = LinearRegression()\n",
    "    lr_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "    return TrainResult(lr_selected, sel_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = {i: train_by_n_features(i) for i in range(1, 8+1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wluduQLZH7wz"
   },
   "source": [
    "Evaluate the predictive performance of the chosen linear models on the data to see how they compare with each other.\n",
    "\n",
    "Produce a plot which shows the root mean squared error for each of the chosen models, both on the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0oiorVRIIoi"
   },
   "outputs": [],
   "source": [
    "train_performance = {idx: res.eval(X_train, y_train) for idx, res in trained_models.items()}\n",
    "test_performance = {idx: res.eval(X_test, y_test) for idx, res in trained_models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(performance, name):\n",
    "    plt.plot(pd.Series(performance))\n",
    "    plt.title(f\"{name}: Mean Squared Error vs N Features\")\n",
    "    plt.xlabel(\"N Features\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.show()\n",
    "\n",
    "plot_performance(train_performance, \"Train\")\n",
    "plot_performance(train_performance, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the feature effects on the different models and show how they differ between themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_features, trained_model in trained_models.items():  \n",
    "    print(f\"{n_features} Features\")\n",
    "    trained_model.plot_feature_effects(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the model and the number of features, the features themselves have different effect on the models. How did the features effect the models? Was there one feature that consistently had a significant effect or was there a combination that showed better predictive performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer above question here**\n",
    "\n",
    "Similar to using the 8 features, the location has presented the best effect on the models. Though it's interesting that the **longitude** alone does not produce a very high effect. Only when it is combined with the **latitude** that you get a very good result.\n",
    "\n",
    "Another observation here is the importance of data normalization. The weights of the linear regression model is impacted by the range of the values of the inputs. This is why despite having pretty good effect on the 8 feature model, the first two features selected by the `SelectFromModel` module were not location related. **AveBedrms** and **MedInc** are two of the lowest when it comes to the data magnitude, and provided very high coefficients despite not having a very good effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvE0odxflAve"
   },
   "source": [
    "# Decision Trees\n",
    "In this part we train a decision tree on a dataset that contains information about cars and their safety class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5qMoRIFOExX"
   },
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChQ1603zOCJi"
   },
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/car/'\n",
    "data_url = url + 'car.data'\n",
    "columns=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "df = pd.read_csv(data_url, header=None)\n",
    "df.columns = columns\n",
    "ct = ColumnTransformer([('categorical', OrdinalEncoder(), df.columns)])\n",
    "ct.fit(df)\n",
    "df = ct.transform(df)\n",
    "df = pd.DataFrame(df, columns=columns)\n",
    "\n",
    "X = df.drop(['class'], axis=1)\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "\n",
    "# dtreeviz does not like if the target class is not of int type, casting the targets to ints.\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5Cz3-Z6rBdU"
   },
   "source": [
    "Here we train the tree with a maximum depth of 4. Play around with the depth to see how the interpretability of the tree changes as the depth increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXO_uuOdQ_8o"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tree_depth=7\n",
    "gini_tree = DecisionTreeClassifier(criterion='gini', max_depth=tree_depth)\n",
    "gini_tree.fit(X_train.values, y_train.values)\n",
    "print(classification_report(y_test.values, gini_tree.predict(X_test.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how the tree is built and how data is divided at each leaf, we can use both graphviz and dtreeviz to visualize the produced rules. It is an easier and more pleasant view of the tree model compared to written rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "FRJ2pfD_vKZC",
    "outputId": "488ddaa5-b537-4a63-9da4-17b6d07045bc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "g = tree.export_graphviz(gini_tree, \n",
    "                         feature_names=X_train.columns.tolist(),\n",
    "                         filled=True)\n",
    "graphviz.Source(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtreeviz\n",
    "\n",
    "viz = dtreeviz.model(gini_tree, X_train, y_train, \n",
    "                     target_name='class', \n",
    "                     feature_names=X_train.columns.tolist(), \n",
    "                     class_names=df['class'].unique().tolist())\n",
    "viz.view()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the heigt of the tree it is commonly so that the predictive performance increases, but as you saw it becomes quite messy quite quickly when you increase the height. \n",
    "\n",
    "Taking both interpretability into account and the predictive performance of the model, at which tree height did you find to strike a good balance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer above question here**\n",
    "\n",
    "Performance is the biggest bottle neck, since in tree depth less than 7, the model cannot predict some of the classes on the test set. Assuming that having a balance on the labels is very much important, this is the minimum acceptable performance. However, 7 is already very complex from a explainability perspective. I would say this is the best balance since this is the lowest depth with acceptable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej_yQLd-3WkR"
   },
   "source": [
    "# Naive Bayes Classifier\n",
    "In this part of the lab we will produce a Naive Bayes model on a spam dataset. The model should then be used to determine if the message is regarded as spam (malicious) or ham (benevolent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIJ43JJc3Y6D"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset downloaded from https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\n",
    "spam_df = pd.read_csv('../data/SMSSpamCollection.csv')\n",
    "spam_df['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(spam_df['text'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, spam_df['spam'], test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify 10 words that you want to investigate. The words are available in the list below when you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just select which ever 10 words\n",
    "\n",
    "import itertools\n",
    "target_words = list(itertools.islice(cv.vocabulary_.keys(), 10))\n",
    "target_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the individual probabilities for the 10 words you have chosen and explain how they impact the model. In the cell below we identify the mapping of the word hello and gather its probability in the form \\[ham, spam\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cv.transform(target_words)\n",
    "nb.predict_proba(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain the words impact on the model based on the probabilities above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the 10 words that most indicate it to be a spam email and the 10 words that mostly indicate for not being spam. Do they make sense?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "The words used for spam indicator makes sense given that most spam emails always provide some sort of compensation for you to act on something. However, the non words are mostly just gibberish and are probably not considered spam just by the rarity that these words appear on emails. They probably don't appear as well on the example spams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs_proba = nb.predict_proba(cv.transform(list(cv.vocabulary_.keys())))\n",
    "vocabs = pd.DataFrame({\n",
    "    \"words\": list(cv.vocabulary_.keys()),\n",
    "    \"ham\": vocabs_proba[:, 0],\n",
    "    \"spam\": vocabs_proba[:, 1],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.sort_values(\"ham\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.sort_values(\"spam\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiTs-ZgDvroV"
   },
   "source": [
    "# Final reflection\n",
    "What have you learned throughout this lab? This includes but is not limited to the example points below.\n",
    "\n",
    "- Are the models that we presented interpretable under any condition? \n",
    "\n",
    "- Would, for instance, the predictions from decision tree with 1000 levels be easy to decipher? \n",
    "\n",
    "- Would a linear regression model with 123456 attributes be interpretable?\n",
    "\n",
    "Reason and reflect about the interpretable models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "The models are interpretable within a certain level of complexity. Once these models become too complex or too large, they also lose their ability to be comprehensible. That being said, these models still provide some way to give us insights even if they are too complex. For example, linear regression still provide the weights as a summary of interpretability while decision trees and forest approach can provide their weight importances. Similarly bayesian models can be sampled to check if their predictions makes sense.\n",
    "\n",
    "In summary, interpretable models can be difficult to understand once their complexity grows, but they still provide some avenues to gain insights on their decision making process."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
